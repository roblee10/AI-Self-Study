# Scheduler

# PyTorch Scheduler

PyTorch Scheduler는 다양한 하이퍼파라미터를 조정하여 최적의 성능을 얻을 수 있습니다. 이러한 하이퍼파라미터에는 초기 학습률, 최종 학습률, 스케줄링 간격 등이 포함됩니다.

### 문제점

- 학습 도중 local minimum 에 갇히거나, 빠른 수렴을 위해서 learning rate 조절 필요, 처음엔 크게하고 최적값에 가까워 질수록 미세 조정 해야 함

### 방법

- lr 처음엔 크게 조절하고 최적값에 가까워 질수록 미세 조정함. Epoch 마다 scheduler.step() 하여 lr 조절 (Optimizer 는 매 batch 마다 optimizer.step() 함)

### 효과

- 학습 성능 향상

### 한계

- 언제 lr을 줄이고 늘릴지 아는것이 쉽지 않음. 상황에 따라 여러 Scheduler 가 존재

### 사용법

 torch.optim.lr_scheduler.ReduceLROnPlateau( PARAMETERS)

- 공통 parameters
    - last_epoch: default = -1 로 설정하게 되면 모델 저장후 학습 재개시 initial_lr 이 lr 이 된다
    - verbose: default = False 로 True 로 설정하게 되면 update 될 때 메세지를 출력한다.

## 대표적인 Scheduler

### StepLR

- step size 마다 gamma 비율로 lr를 감소시킨다

![Untitled](Scheduler%202a8a5e435c0f4a24859755f396d4a105/Untitled.png)

### Parameters

- optimizer: optimizer 정의
- step_size: 몇 epoch 마다 lr을 감소시킬지 결정
- gamma: lr 을 gamma 만큼의 비율로 감소시킨다.

### MultiStepLR

- learning rate 를 감소시킬 epoch를 직접 지정한다

![Untitled](Scheduler%202a8a5e435c0f4a24859755f396d4a105/Untitled%201.png)

- Parameters
    - optimizer: optimizer 정의
    - milestones: lr을 감소시킬 epoch index 의 리스트
    - gamma: lr 을 gamma 만큼의 비율로 감소시킨다.

### ReduceLROnPlateau

- 성능 향상이 없을 때 learning rate를 감소시킨다.
- 따라서 validation loss 와 같은 평가지표를 input으로 제공해야 한다.
- Parameters
    - optimizer: optimizer 정의
    - mode: min 혹은 max  둘 중 하나의 모드로 설정. min 은 평가지표가 감소를 멈출 때, max 는 평가 지표가 증가를 멈출 때
    - factor: lr 감소시킬 비율 설정 (lr*factor)
    - patience: metric이 향상 안될 때, 몇 epoch 를 참을 것인가 설정
    - threshold, threshold_mode
    - cool_down : lr이 감소한 후 몇 epoch 동안 lr scheduler 동장을 쉴 지
    - min_lr : 감소 시킬 최소 lr
    - eps: 줄이기 전 과 줄인 후 의 lr 차이가 eps 보다 작으면 scheduler 무시한다.